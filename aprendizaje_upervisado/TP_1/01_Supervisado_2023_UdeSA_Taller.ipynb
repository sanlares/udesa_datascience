{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFsTQ9KQrb58"
      },
      "source": [
        "# Aprendizaje Supervisado I\n",
        "# UdeSA, 2023\n",
        "## Profesores: Marcela Svarc, Juan Manuel Pérez\n",
        "\n",
        "\n",
        "## Taller 1: Algoritmos de clasificación + evaluación\n",
        "\n",
        "\n",
        "**Fecha de entrega: 05/10/2023, 23:59 hs**\n",
        "\n",
        "Enviar individualmente a perezj@udesa.edu.ar con subject [TALLER-SUPERVISADO] <APELLIDO>\n",
        "\n",
        "\n",
        "\n",
        "Este notebook contiene los ejercicios del taller 1. Los mismos deberán ser realizados usando Python. Pueden hacerlo en colab como en sus propias computadoras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rk-rKVGBrdR9"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbwpll_ftnPy"
      },
      "source": [
        "# 1. Métricas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tykrFXZYkXyc"
      },
      "source": [
        "### 1.1 Programar las siguientes métricas como funciones de python con la siguiente aridad:\n",
        "\n",
        "\n",
        "```python\n",
        "def metrica(y_true,y_pred):\n",
        "  ...\n",
        "  return score\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "1.   accuracy\n",
        "2.   precision\n",
        "3.   recall\n",
        "4.   F1-score\n",
        "5.   F $_{\\beta}$-score\n",
        "6.   Matriz de confusión\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gqMwT2o5kZaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4, 2],\n",
              "       [2, 4]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    x=(y_true==y_pred)\n",
        "    return x.mean()\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    tp=(y_true==y_pred) & (y_pred==1)\n",
        "    tp=tp.sum()\n",
        "    fp=(y_true==0) & (y_pred==1)\n",
        "    fp=fp.sum()\n",
        "    pr=tp/(tp+fp)\n",
        "    return pr\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    tp=(y_true==y_pred) & (y_pred==1)\n",
        "    tp=tp.sum()\n",
        "    fn=(y_true==1) & (y_pred==0)\n",
        "    fn=fn.sum()\n",
        "    rc=tp/(tp+fn)\n",
        "    return rc\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    nominador=precision(y_true, y_pred)*recall(y_true, y_pred)\n",
        "    denominador=precision(y_true, y_pred)+recall(y_true, y_pred)\n",
        "    ef1=(2*nominador)/denominador\n",
        "    return ef1\n",
        "\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    tp=(y_true==y_pred) & (y_pred==1)\n",
        "    tp=tp.sum()\n",
        "    fp=(y_true==0) & (y_pred==1)\n",
        "    fp=fp.sum()\n",
        "    fn=(y_true==1) & (y_pred==0)\n",
        "    fn=fn.sum()\n",
        "    matrix=np.array([[tp, fp],[fn,tp]])  \n",
        "    return matrix\n",
        "\n",
        "confusion_matrix(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdrqFrEXoLEO"
      },
      "source": [
        "### 1.2 Crear diversos casos de test que prueben diferentes aspectos de cada metrica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "CWdaJjPDoSyk",
        "outputId": "55e3f397-7f02-4aee-d429-afe18a9388f6"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m y_true \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39misclose(accuracy(y_true, y_pred), \u001b[39m7\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TODO: Crear tests usando asserts\n",
        "\"\"\"\n",
        "\n",
        "y_true = np.array([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1])\n",
        "y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1])\n",
        "\n",
        "assert np.isclose(accuracy(y_true, y_pred), 7/10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKLwxPifjJev"
      },
      "source": [
        "## Clasificadores\n",
        "\n",
        "Vamos a implementar los clasificadores usando el mismo esquema de la librería `sklearn`. Para ello, vamos a crear una clase `Clasificador` que tenga los siguientes métodos:\n",
        "\n",
        "*   `fit(X,y)`: recibe los datos de entrenamiento y los labels y entrena el clasificador\n",
        "*   `predict(X)`: recibe los datos de test y devuelve las predicciones\n",
        "\n",
        "Empecemos con clasificadores baseline, es decir, clasificadores que no aprenden nada o casi nada de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-oOsGhcoTY0"
      },
      "source": [
        "### 1.3 Random\n",
        "\n",
        "Implementar un clasificador que prediga una clase al azar. Para ello, usar la función `np.random.randint`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvsP6fDq7QSj"
      },
      "outputs": [],
      "source": [
        "class RandomClassifier:\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO: Completar\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Completar\n",
        "        X va a ser un np.array de tamaño (n,p)\n",
        "        n entradas y p predictores\n",
        "        Que queremos devolver? un np.array de tamaño n\n",
        "        \"\"\"\n",
        "        n=X.shape[0]\n",
        "        return np.random.randint(low=0,high=self.num_classes, size=n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aduEFp5xjJex"
      },
      "outputs": [],
      "source": [
        "# Prueba: genero 1000 etiquetas con 0 y 1000 etiquetas con 1\n",
        "# Un clasificador aleatorio debería tener un accuracy cercano a 0.5 (¿por qué?)\n",
        "X=np.random.randint(0,2,size=(200,2))\n",
        "y_true = np.concatenate([np.zeros(1000), np.ones(1000)])\n",
        "\n",
        "clf=RandomClassifier(2)\n",
        "\n",
        "clf.fit(X, y_true)\n",
        "\n",
        "y_pred=clf.predict(X)\n",
        "y_pred = RandomClassifier(2).predict(y_true)\n",
        "assert np.isclose(accuracy_score(y_true, y_pred), 0.5, atol=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lp2JZVljJey"
      },
      "source": [
        "## 1.4 Majority Class Classifier\n",
        "\n",
        "Programar un algoritmo que prediga la clase mayoritaria sobre los datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHoZ0lE7jJey"
      },
      "source": [
        "Mirar la documentación de `np.bincount` y `np.argmax`. ¿Para qué sirven?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ANYT0cI9jJey"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 1 3 2 8]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = np.array([1,0,0,2,2,2,3,3,4,4,4,4,4,4,4,4])\n",
        "c=np.bincount(y)\n",
        "print(c)\n",
        "np.argmax(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "aRBoXQp5jJey"
      },
      "outputs": [],
      "source": [
        "# el majority classifier es una combinacion de bincount y argmax\n",
        "class MajorityClassifier:\n",
        "    def __init__(self):\n",
        "        self.majority = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        c=np.bincount(y)\n",
        "        self.mc=np.argmax(c)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return(self.mc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'predict'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m modelo\u001b[39m=\u001b[39mMajorityClassifier()\n\u001b[1;32m      4\u001b[0m modelo\u001b[39m=\u001b[39mmodelo\u001b[39m.\u001b[39mfit(X,y)\n\u001b[0;32m----> 5\u001b[0m predict\u001b[39m=\u001b[39mmodelo\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[1;32m      6\u001b[0m predict\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "y=np.array([1,2,2,3,4])\n",
        "X=np.array([0,0,0,1,1])\n",
        "modelo=MajorityClassifier()\n",
        "modelo=modelo.fit(X,y)\n",
        "predict=modelo.predict(X)\n",
        "predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDYPfIbpjJez"
      },
      "source": [
        "## 2. Naive Bayes\n",
        "\n",
        "Supongamos que tenemos un problema de clasificación con $K$ clases, y $p$ predictores/features.\n",
        "\n",
        "Recordemos que por el teorema de Bayes, podemos escribir la probabilidad de que una instancia $x$ pertenezca a una clase $k$ (posterior) como:\n",
        "\n",
        "$$P(y=k|x) = \\frac{P(x|y=k)P(y=k)}{P(x)}$$\n",
        "\n",
        "- $P(x|y=k)$ es la probabilidad condicional de $x$ dado que la clase es $k$. Podemos escribirla como $f_k(x)$, donde $f_k$ es la función de densidad de probabilidad de la clase $k$.\n",
        "- $P(y=k)$ lo podemos modelar como la frecuencia de la clase $k$ en el conjunto de entrenamiento (usualmente, escrita como $\\pi_k$)\n",
        "- $P(x)$ es la probabilidad marginal de $x$, que podemos calcular como la suma de las probabilidades condicionales de $x$ para cada clase $k$ (Probabilidad total)\n",
        "$$P(x) = \\sum_{k=1}^K P(x|y=k)P(y=k)$$\n",
        "\n",
        "Reescribiendo todo esto, podemos calcular la probabilidad de que una instancia $x$ pertenezca a una clase $k$ como:\n",
        "\n",
        "$$P(y=k|x) = \\frac{f_k(x)\\pi_k}{\\sum_{j=1}^K f_j(x)\\pi_j}$$\n",
        "\n",
        "$x$ es un vector de $p$ dimensiones, y cada una de sus componentes es una variable aleatoria. Si asumimos que cada componente de $x$ es independiente de las demás, podemos escribir la función de densidad de probabilidad de la clase $k$ como:\n",
        "\n",
        "$$f_k(x) = \\prod_{j=1}^p f_{kj}(x_j)$$\n",
        "\n",
        "Entonces, para cada clase $k$, tenemos que estimar $p$ funciones de densidad de probabilidad $f_{kj}(x_j)$, una para cada predictor $j$ (tenemos $K \\times p$ predictores)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4afir-kjJez"
      },
      "source": [
        "## 3.1 Precalentamiento\n",
        "\n",
        "### 3.1.2 Estimar funcion de densidad para variables categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0_FFKp9jJez"
      },
      "source": [
        "Sea $X$ un vector de observaciones de una variable Bernoulli. Utilizar `np.bincount` para estimar la función de densidad de probabilidad de $X$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JXIqKl2jJe0"
      },
      "outputs": [],
      "source": [
        "X = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "# TODO: Completar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC5DHpkpjJe0"
      },
      "source": [
        "Utilizar `scipy.stats.rv_discrete` para estimar la función de densidad de probabilidad de $X$\n",
        "\n",
        "La función recibe un parámetro `values` que toma un par de arrays `(Xk, pk)` donde `Xk` son los valores que toma la variable aleatoria y `pk` son las probabilidades asociadas a cada valor.\n",
        "\n",
        "Utilizar la función `.pmf` para evaluar la función de densidad de probabilidad en un punto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxmb0J87jJe1"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "probas = np.bincount(X) / X.shape[0]\n",
        "\n",
        "values = np.unique(X)\n",
        "\n",
        "F = scipy.stats.rv_discrete(values=(values, probas))\n",
        "\n",
        "F.pmf(0), F.pmf(1), F.pmf(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx26Gun_Iq-D"
      },
      "source": [
        "### 3.1 Programar naive bayes para variables categoricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqxwYW1bjJe4"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "class NaiveBayes:\n",
        "    \"\"\"\n",
        "    Naive bayes para variables categóricas\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.priors = None\n",
        "        self.densities = None\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Entrenar Naive Bayes categórico\n",
        "\n",
        "        Argumentos:\n",
        "        ----------\n",
        "\n",
        "        X: array de numpy de forma (n_samples, n_features)\n",
        "            Datos de entrenamiento. Cada feature debe ser una variable categórica\n",
        "        y: array de variables categóricas\n",
        "            Etiquetas de las clases\n",
        "        \"\"\"\n",
        "\n",
        "        assert X.dtype == int, \"X debe ser int\"\n",
        "\n",
        "        \"\"\"\n",
        "        Una pequeña ayudita para entender el código:\n",
        "\n",
        "        Hay que calcular dos cosas:\n",
        "\n",
        "        - Las probabilidades a priori de cada clase\n",
        "        - Las funciones de densidad de cada variable categórica X_1, ... X_n para cada clase\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Calcular prior\n",
        "        self.priors = np.bincount(y) / len(y)\n",
        "        # TODO: calcular cantidad de clases (usar np.unique)\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        num_features = X.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Calcular Las funciones de densidad de cada variable categórica X_1, ... X_n para cada clase\n",
        "        # Armo una matriz de n_classes x num_features que sería la cantidad de f_ij que necesito\n",
        "        self.densities = [[None] * num_features for i in range(n_classes)]\n",
        "\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            \"\"\"\n",
        "            TODO: Completar\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predecir las etiquetas de las clases para cada dato de X\n",
        "        \"\"\"\n",
        "        assert self.priors is not None, \"Hay que llamar a fit primero\"\n",
        "\n",
        "        \"\"\"\n",
        "        TODO: Completar\n",
        "        \"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LjYY3udjJe4"
      },
      "source": [
        "Vamos a probar todo esto en un dataset generado aleatoriamente. Este dataset consta de 200 observaciones con 5 variables Bernoulli $X_1, X_2, X_3, X_4, X_5$\n",
        "\n",
        "$$Y|X_1, X_2, X_3, X_4, X_5 \\sim Bernoulli(\\frac{X_1 + \\ldots + X_5}{5})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Piq4DznijJe5"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_samples = 200\n",
        "num_vars = 5\n",
        "\n",
        "np.random.seed(2023)\n",
        "\n",
        "X = np.random.randint(0, 2, (n_samples, num_vars))\n",
        "\n",
        "# Creamos  Bernoulli(P/num_vars)\n",
        "\n",
        "P = X.sum(axis=1) / num_vars\n",
        "y = np.random.binomial(1, P)\n",
        "\n",
        "# Train / test split\n",
        "\n",
        "X_train, y_train = X[:160], y[:160]\n",
        "X_test, y_test = X[160:], y[160:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raKjltOCjJe5"
      },
      "source": [
        "Entrenar el modelo sobre los datos de entrenamiento y predecir sobre los datos de test para:\n",
        "\n",
        "- RandomClassifier\n",
        "- MajorityClassifier\n",
        "- NaiveBayes\n",
        "\n",
        "Reportar accuracy, precision, recall y F1-score para cada clasificador\n",
        "\n",
        "\n",
        "\n",
        "- ¿Anda mejor Naive Bayes que los clasificadores baseline? ¿se cumplen las hipótesis de Naive Bayes en este caso?\n",
        "\n",
        "- **OPCIONAL**. Calcular el error de Bayes. ¿Cómo se compara con el de Naive Bayes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18YLl2vQjJe6"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation between the sum and Y\n",
        "\n",
        "clf = NaiveBayes()\n",
        "\n",
        "\"\"\"\n",
        "TODO: Completar\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TldnN4OYjJe6"
      },
      "outputs": [],
      "source": [
        "clf = RandomClassifier(2)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TODO: Completar\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2erQizCjJe7"
      },
      "outputs": [],
      "source": [
        "clf = MajorityClassifier()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TODO: Completar\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zUcPPtPKevV"
      },
      "source": [
        "### 3.2 Programar naive bayes para variables gaussianas.\n",
        "¿Cómo podria estimar $P(a_i|v_j)$? Piense en fitear diferentes distribuciones que le permitan luego inferir valores nuevos.\n",
        "\n",
        "Ayuda: Puede usera `scipy.stats.norm( mu , std ).pdf( new_val )` como funcion de densidad de probabilidad de una normal y evaluarla en new_val\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIWXzzuNjJe7"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayes:\n",
        "    \"\"\"\n",
        "    Naive bayes para variables gaussianas\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.priors = None\n",
        "        self.densities = None\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Entrenar Naive Bayes gaussiano\n",
        "\n",
        "        Argumentos:\n",
        "        ----------\n",
        "\n",
        "        X: array de numpy de forma (n_samples, n_features)\n",
        "            Datos de entrenamiento. Cada feature debe ser una variable categórica\n",
        "        y: array de variables categóricas\n",
        "            Etiquetas de las clases\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Una pequeña ayudita para entender el código:\n",
        "\n",
        "        Hay que calcular dos cosas:\n",
        "\n",
        "        - Las probabilidades a priori de cada clase\n",
        "        - Las funciones de densidad de cada variable categórica X_1, ... X_n para cada clase\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        TODO: Completar\n",
        "        \"\"\"\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class for each sample in X\n",
        "\n",
        "        Arguments:\n",
        "        ----------\n",
        "\n",
        "        X: numpy array of shape (n_samples, n_features)\n",
        "            Training data. Each feature must be a categorical variable\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "\n",
        "        y_pred: numpy array of shape (n_samples,)\n",
        "            Predicted class for each sample\n",
        "        \"\"\"\n",
        "        assert self.priors is not None, \"You must call fit before predict\"\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        TODO: Completar\n",
        "        \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZpAFQRjJe7"
      },
      "source": [
        "### 3.3 LDA\n",
        "\n",
        "Programar LDA para variables gaussianas\n",
        "\n",
        "Utilizar las funciones `np.mean` y `np.cov` para estimar los parámetros de las gaussianas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No6BnGDtjJe8"
      },
      "outputs": [],
      "source": [
        "class LDAClassifier:\n",
        "    def __init__(self):\n",
        "        self.distributions = None\n",
        "        self.priors = None\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO: Completar\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Completar\n",
        "        \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTAUAZL_jJe8"
      },
      "source": [
        "#### Opcional: Implementar QDA\n",
        "\n",
        "Leer en el ISLP el capítulo de QDA y programar el algoritmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpmRY34tjJe8"
      },
      "source": [
        "## 3.4 Dataset Iris (Lirios)\n",
        "\n",
        "\n",
        "![texto alternativo](https://cdn0.ecologiaverde.com/es/posts/4/1/0/tipos_de_lirios_3014_orig.jpg)\n",
        "\n",
        "El dataset Iris es un dataset clásico de clasificación. Contiene 150 observaciones de 3 clases de flores distintas (50 observaciones por clase). Cada observación tiene 4 features: largo y ancho del sépalo y largo y ancho del pétalo.\n",
        "\n",
        "Fue introducido por el estadístico y biólogo británico Ronald Fisher en 1936.\n",
        "\n",
        "Dato importante: si tienen gatos, no tengan lirios cerca. Son sumamente tóxicos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1RRwriBjJe8"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "df = pd.DataFrame(X, columns=[x.strip(\" (cm)\") for x in iris.feature_names])\n",
        "df[\"species\"] = [iris.target_names[x] for x in y]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPzFf6j0jJe9"
      },
      "source": [
        "### 3.4.1 Análisis exploratorio\n",
        "\n",
        "Explorar el dataset. ¿Qué tipo de problema es? ¿Qué tipo de features tiene?\n",
        "\n",
        "Use `sns.displot` y `sns.pairplot` para visualizar las distribuciones de las variables y las relaciones entre ellas.\n",
        "\n",
        "¿Qué clase es más fácil de separar de las otras dos? ¿Cuál es más difícil?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Xf7fgcjJe9"
      },
      "source": [
        "### 3.4.2 Split train/test\n",
        "\n",
        "Utilice la función `train_test_split` de `sklearn.model_selection` para dividir los datos en train y test. Utilice un 66% para train y un 33% para test.\n",
        "\n",
        "Setee como `random_state` el número 42 (¿para qué sirve?). Usar `stratify` para que los datos se dividan de manera estratificada (¿qué es esto?)\n",
        "\n",
        "\n",
        "Consultar [la documentación de `train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) para ver cómo se usa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB5ac-StjJe9"
      },
      "outputs": [],
      "source": [
        "# Train / test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpAfwHRpjJfD"
      },
      "outputs": [],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlyK7XX-jJfD"
      },
      "source": [
        "### 3.4.3 Entrenamiento de modelos\n",
        "\n",
        "Entrenar los modelos que ya programamos y predecir sobre los datos de test. Reportar accuracy, precision, recall y F1-score para cada clasificador (pueden usar `sklearn.metrics.classification_report`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c8RitnbjJfD"
      },
      "source": [
        "## 3.4.4 Análisis de errores\n",
        "\n",
        "Calcular la matriz de confusión para el mejor clasificador. ¿Qué clase se confunde más con qué clase? ¿Por qué?\n",
        "\n",
        "Usar `sklearn.metrics.confusion_matrix`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsZ0C86yjJfE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0T16x7ZjJfE"
      },
      "source": [
        "## 4. KNN\n",
        "\n",
        "### 4.1 Implementar KNN\n",
        "\n",
        "Implementar un clasificador KNN. Pasar como parámetro $k$ en el constructor de la clase.\n",
        "\n",
        "Correr sobre el dataset Iris y reportar accuracy, precision, recall y F1-score. ¿Cómo se compara con los clasificadores anteriores?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
